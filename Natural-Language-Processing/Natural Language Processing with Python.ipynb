{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting Hacker News upvotes using headlines. Learn some of the basic building blocks of natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt url](https://tctechcrunch2011.files.wordpress.com/2013/05/hacker-news1.jpg?w=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Hacker News](http://news.ycombinator.com/) is a community where users can submit articles, and other users can upvote those articles. The articles with the most upvotes make it to the front page, where they're more visible to the community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set consists of submissions users made to Hacker News from 2006 to 2015. Developer Arnaud Drizard used the Hacker News API to scrape the data, which can find in one of [his GitHub repositories](https://github.com/arnauddri/hn). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading In The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data and sampled 3000 rows randomly from the data, and removed all of the extraneous columns. \n",
    "\n",
    "The data only has four columns:\n",
    "\n",
    "    •\tsubmission_time - When the article was submitted\n",
    "    •\tupvotes - The number of upvotes the article received\n",
    "    •\turl - The base URL of the article\n",
    "    •\theadline - The article's headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_time</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>url</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-02-20T11:29:58.000Z</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ask HN: Simple SaaS as first Golang web app?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-02-20T11:34:22.000Z</td>\n",
       "      <td>1</td>\n",
       "      <td>startupjuncture.com</td>\n",
       "      <td>24sessions: live business advice over video-chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-20T11:35:32.000Z</td>\n",
       "      <td>3</td>\n",
       "      <td>blog.erratasec.com</td>\n",
       "      <td>Some notes on SuperFish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-02-20T11:36:18.000Z</td>\n",
       "      <td>1</td>\n",
       "      <td>twitter.com</td>\n",
       "      <td>Apple Watch models could contain 29.16g of gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-20T11:41:06.000Z</td>\n",
       "      <td>1</td>\n",
       "      <td>phpconference.co.uk</td>\n",
       "      <td>PHP UK Conference Diversity Scholarship Programme</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            submission_time  upvotes                  url  \\\n",
       "0  2015-02-20T11:29:58.000Z        2                  NaN   \n",
       "1  2015-02-20T11:34:22.000Z        1  startupjuncture.com   \n",
       "2  2015-02-20T11:35:32.000Z        3   blog.erratasec.com   \n",
       "3  2015-02-20T11:36:18.000Z        1          twitter.com   \n",
       "4  2015-02-20T11:41:06.000Z        1  phpconference.co.uk   \n",
       "\n",
       "                                            headline  \n",
       "0       Ask HN: Simple SaaS as first Golang web app?  \n",
       "1   24sessions: live business advice over video-chat  \n",
       "2                            Some notes on SuperFish  \n",
       "3    Apple Watch models could contain 29.16g of gold  \n",
       "4  PHP UK Conference Diversity Scholarship Programme  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "submissions = pd.read_csv(\"C:/Users/i7/csv/stories.csv\", header=None)\n",
    "submissions = submissions.drop([0, 2, 3, 6], 1)\n",
    "submissions.columns = ['submission_time', 'upvotes', 'url', 'headline']\n",
    "submissions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1553934, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "shuffled_idx = np.random.permutation(submissions.index)\n",
    "submissions = submissions.loc[shuffled_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_time</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>url</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>404493</th>\n",
       "      <td>2013-09-26T12:11:48Z</td>\n",
       "      <td>4</td>\n",
       "      <td>youtube.com</td>\n",
       "      <td>Bill Gates interview at Harvard (2013)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435421</th>\n",
       "      <td>2009-05-08T14:36:28Z</td>\n",
       "      <td>2</td>\n",
       "      <td>news.bbc.co.uk</td>\n",
       "      <td>Google boss won't quit Apple job</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207322</th>\n",
       "      <td>2011-02-15T18:44:58Z</td>\n",
       "      <td>1</td>\n",
       "      <td>n-rhman.com</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756837</th>\n",
       "      <td>2012-07-23T03:07:44Z</td>\n",
       "      <td>93</td>\n",
       "      <td>spectrum.ieee.org</td>\n",
       "      <td>Why Bad Jobs-or No Jobs-Happen to Good Workers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639885</th>\n",
       "      <td>2012-12-19T19:50:57Z</td>\n",
       "      <td>2</td>\n",
       "      <td>charlespetzold.com</td>\n",
       "      <td>First-Person Shooter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              submission_time  upvotes                 url  \\\n",
       "404493   2013-09-26T12:11:48Z        4         youtube.com   \n",
       "1435421  2009-05-08T14:36:28Z        2      news.bbc.co.uk   \n",
       "1207322  2011-02-15T18:44:58Z        1         n-rhman.com   \n",
       "756837   2012-07-23T03:07:44Z       93   spectrum.ieee.org   \n",
       "639885   2012-12-19T19:50:57Z        2  charlespetzold.com   \n",
       "\n",
       "                                               headline  \n",
       "404493           Bill Gates interview at Harvard (2013)  \n",
       "1435421                Google boss won't quit Apple job  \n",
       "1207322                                                  \n",
       "756837   Why Bad Jobs-or No Jobs-Happen to Good Workers  \n",
       "639885                             First-Person Shooter  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions = submissions.drop(submissions.index[3000:])\n",
    "submissions.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2797 entries, 404493 to 584256\n",
      "Data columns (total 4 columns):\n",
      "submission_time    2797 non-null object\n",
      "upvotes            2797 non-null int64\n",
      "url                2797 non-null object\n",
      "headline           2797 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 109.3+ KB\n"
     ]
    }
   ],
   "source": [
    "submissions = submissions.dropna()\n",
    "submissions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Model to Tokenize the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert each headline to numerical representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Bill', 'Gates', 'interview', 'at', 'Harvard', '(2013)'], ['Google', 'boss', \"won't\", 'quit', 'Apple', 'job']]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Tokenization\n",
    "# Split each headline into individual words on the space character(\" \"), \n",
    "\n",
    "tokenized_headlines = []\n",
    "for item in submissions[\"headline\"]:\n",
    "    tokenized_headlines.append(item.split(\" \"))\n",
    "    \n",
    "print(tokenized_headlines[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['bill', 'gates', 'interview', 'at', 'harvard', '2013'], ['google', 'boss', 'wont', 'quit', 'apple', 'job']]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Lowercasing and removing punctuation\n",
    "\n",
    "punctuations_list = [\",\", \":\", \";\", \".\", \"'\", '\"', \"’\", \"?\", \"/\", \"-\", \"+\", \"&\", \"(\", \")\"]\n",
    "clean_tokenized = []\n",
    "for item in tokenized_headlines:\n",
    "    tokens = []\n",
    "    for token in item:\n",
    "        token = token.lower()\n",
    "        for punc in punctuations_list:\n",
    "            token = token.replace(punc, \"\")\n",
    "        tokens.append(token)\n",
    "    clean_tokenized.append(tokens)\n",
    "\n",
    "print(clean_tokenized[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3: Retrieve all of the unique words from all of the headlines\n",
    "# unique_tokens contains any tokens that occur more than once across all of the headlines.\n",
    "\n",
    "unique_tokens = []\n",
    "single_tokens = []\n",
    "for tokens in clean_tokenized:\n",
    "    for token in tokens:\n",
    "        if token not in single_tokens:\n",
    "            single_tokens.append(token)\n",
    "        elif token in single_tokens and token not in unique_tokens:\n",
    "            unique_tokens.append(token)\n",
    "\n",
    "counts = pd.DataFrame(0, index=np.arange(len(clean_tokenized)), columns=unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Counting Token Occurrences\n",
    "\n",
    "for i, item in enumerate(clean_tokenized):\n",
    "    for token in item:\n",
    "        if token in unique_tokens:\n",
    "            counts.iloc[i][token] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Columns To Increase Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too many columns will cause the model to fit to noise instead of the signal in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove any words that occur fewer than 5 times or more than 100 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2797 entries, 0 to 2796\n",
      "Columns: 2317 entries,  to stranger\n",
      "dtypes: int64(2317)\n",
      "memory usage: 49.5 MB\n"
     ]
    }
   ],
   "source": [
    "counts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts = counts.sum(axis=0)\n",
    "counts = counts.loc[:,(word_counts >= 5) & (word_counts <= 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2797 entries, 0 to 2796\n",
      "Columns: 662 entries, bill to bug\n",
      "dtypes: int64(662)\n",
      "memory usage: 14.1 MB\n"
     ]
    }
   ],
   "source": [
    "counts.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train-test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(counts, submissions[\"upvotes\"], test_size=0.2, random_state=1)\n",
    "\n",
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# instantiate an instance\n",
    "clf = LinearRegression()\n",
    "\n",
    "# Fit the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_predict = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Prediction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.6671083116\n"
     ]
    }
   ],
   "source": [
    "mse = sum((y_predict - y_test) ** 2) / len(y_predict)\n",
    "rmse = (mse)**0.5\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The calculation have high error (rmse of ~50 upvotes) in predicting upvotes as  used a very small data set. With larger training sets, this should decrease dramatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_time</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>url</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>404493</th>\n",
       "      <td>2013-09-26T12:11:48Z</td>\n",
       "      <td>4</td>\n",
       "      <td>youtube.com</td>\n",
       "      <td>Bill Gates interview at Harvard (2013)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435421</th>\n",
       "      <td>2009-05-08T14:36:28Z</td>\n",
       "      <td>2</td>\n",
       "      <td>news.bbc.co.uk</td>\n",
       "      <td>Google boss won't quit Apple job</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207322</th>\n",
       "      <td>2011-02-15T18:44:58Z</td>\n",
       "      <td>1</td>\n",
       "      <td>n-rhman.com</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756837</th>\n",
       "      <td>2012-07-23T03:07:44Z</td>\n",
       "      <td>93</td>\n",
       "      <td>spectrum.ieee.org</td>\n",
       "      <td>Why Bad Jobs-or No Jobs-Happen to Good Workers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639885</th>\n",
       "      <td>2012-12-19T19:50:57Z</td>\n",
       "      <td>2</td>\n",
       "      <td>charlespetzold.com</td>\n",
       "      <td>First-Person Shooter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              submission_time  upvotes                 url  \\\n",
       "404493   2013-09-26T12:11:48Z        4         youtube.com   \n",
       "1435421  2009-05-08T14:36:28Z        2      news.bbc.co.uk   \n",
       "1207322  2011-02-15T18:44:58Z        1         n-rhman.com   \n",
       "756837   2012-07-23T03:07:44Z       93   spectrum.ieee.org   \n",
       "639885   2012-12-19T19:50:57Z        2  charlespetzold.com   \n",
       "\n",
       "                                               headline  \n",
       "404493           Bill Gates interview at Harvard (2013)  \n",
       "1435421                Google boss won't quit Apple job  \n",
       "1207322                                                  \n",
       "756837   Why Bad Jobs-or No Jobs-Happen to Good Workers  \n",
       "639885                             First-Person Shooter  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions_data = pd.read_csv(\"C:/Users/i7/csv/stories.csv\", header=None)\n",
    "submissions_data = submissions_data.drop([0, 2, 3, 6], 1)\n",
    "submissions_data.columns = ['submission_time', 'upvotes', 'url', 'headline']\n",
    "\n",
    "np.random.seed(1)\n",
    "shuffled_idx = np.random.permutation(submissions_data.index)\n",
    "submissions_data = submissions_data.loc[shuffled_idx]\n",
    "\n",
    "submissions_data = submissions_data.drop(submissions_data.index[10000:])\n",
    "\n",
    "submissions_data = submissions_data.dropna()\n",
    "submissions_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9373 entries, 404493 to 1019647\n",
      "Data columns (total 4 columns):\n",
      "submission_time    9373 non-null object\n",
      "upvotes            9373 non-null int64\n",
      "url                9373 non-null object\n",
      "headline           9373 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 366.1+ KB\n"
     ]
    }
   ],
   "source": [
    "submissions_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Matrix for all the Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix.todense():\n",
      " [[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "full_matrix.shape:\n",
      " (9373, 13873)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Construct a bag of words matrix.\n",
    "# This will lowercase everything, and ignore all punctuation by default.\n",
    "# It will also remove stop words.\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words=\"english\")\n",
    "\n",
    "matrix = vectorizer.fit_transform(submissions_data[\"headline\"])\n",
    "# Created bag of words matrix with far fewer commands.\n",
    "print(\"matrix.todense():\\n\", matrix.todense())\n",
    "\n",
    "# Let's apply the same method to all the headlines in all 100000 submissions.\n",
    "# We'll also add the url of the submission to the end of the headline so we can take it into account.\n",
    "submissions_data['full_test'] = submissions_data[\"headline\"] + \" \" + submissions_data[\"url\"]\n",
    "full_matrix = vectorizer.fit_transform(submissions_data[\"headline\"])\n",
    "print(\"full_matrix.shape:\\n\", full_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a subset of the columns that are the most informative -- that is, the columns that differentiate between good and bad headlines the best. A good way to figure out the most informative columns is to use something called a chi-squared test.\n",
    "\n",
    "A chi-squared test finds the words that discriminate the most between highly upvoted posts and posts that weren't upvoted. This can be words that occur a lot in highly upvoted posts, and not at all in posts without upvotes, or words that occur a lot in posts that aren't upvoted, but don't occur in posts that are upvoted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectKBest(k=1000, score_func=<function chi2 at 0x000000000F842840>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Convert the upvotes variable to binary so it works with a chi-squared test.\n",
    "col = submissions_data[\"upvotes\"].copy(deep=True)\n",
    "col_mean = col.mean()\n",
    "col[col < col_mean] = 0\n",
    "col[(col > 0) & (col > col_mean)] = 1\n",
    "\n",
    "# Find the 1000 most informative columns\n",
    "selector = SelectKBest(chi2, k=1000)\n",
    "selector.fit(full_matrix, col)\n",
    "top_words = selector.get_support().nonzero()\n",
    "\n",
    "# Pick only the most informative columns in the data.\n",
    "chi_matrix = full_matrix[:,top_words[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Meta Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore the \"meta\" features of the headlines we're missing out on a lot of good information. These features are things like length, amount of punctuation, average word length, and other sentence specific features.\n",
    "\n",
    "Adding these in can greatly increase prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# List of functions to apply.\n",
    "transform_functions = [\n",
    "    lambda x: len(x),\n",
    "    lambda x: x.count(\" \"),\n",
    "    lambda x: x.count(\".\"),\n",
    "    lambda x: x.count(\"!\"),\n",
    "    lambda x: x.count(\"?\"),\n",
    "    lambda x: len(x) / (x.count(\" \") + 1),\n",
    "    lambda x: x.count(\" \") / (x.count(\".\") + 1),\n",
    "    lambda x: len(re.findall(\"\\d\", x)),\n",
    "    lambda x: len(re.findall(\"[A-Z]\", x)),\n",
    "]\n",
    "\n",
    "# Apply each function and put the results into a list.\n",
    "columns = []\n",
    "for func in transform_functions:\n",
    "    columns.append(submissions_data[\"headline\"].apply(func))\n",
    "    \n",
    "# Convert the meta features to a numpy array.\n",
    "meta = np.asarray(columns).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding in more Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding submission_time, that tells when a story was submitted, and could add more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = []\n",
    "\n",
    "# Convert the submission dates column to datetime.\n",
    "submissions_data_dates = pd.to_datetime(submissions_data[\"submission_time\"])\n",
    "\n",
    "# Transform functions for the datetime column.\n",
    "transform_functions = [\n",
    "    lambda x: x.year,\n",
    "    lambda x: x.month,\n",
    "    lambda x: x.day,\n",
    "    lambda x: x.hour,\n",
    "    lambda x: x.minute,\n",
    "]\n",
    "\n",
    "# Apply all functions to the datetime column.\n",
    "for func in transform_functions:\n",
    "    columns.append(submissions_data_dates.apply(func))\n",
    "\n",
    "# Convert the meta features to a numpy array.\n",
    "non_nlp = np.asarray(columns).T\n",
    "\n",
    "# Concatenate the features together.\n",
    "features = np.hstack([non_nlp, meta, chi_matrix.todense()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ridge regression to make predictions, ridge regression introduces a penalty on the coefficients, which prevents them from becoming too large. This can help it work with large numbers of predictors (columns) that are correlated to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "import random\n",
    "\n",
    "train_rows = 7500\n",
    "# Set a seed to get the same \"random\" shuffle every time.\n",
    "random.seed(1)\n",
    "\n",
    "# Shuffle the indices for the matrix.\n",
    "indices = list(range(features.shape[0]))\n",
    "random.shuffle(indices)\n",
    "\n",
    "# Create train and test sets.\n",
    "train = features[indices[:train_rows], :]\n",
    "test = features[indices[train_rows:], :]\n",
    "train_upvotes = submissions_data[\"upvotes\"].iloc[indices[:train_rows]]\n",
    "test_upvotes = submissions_data[\"upvotes\"].iloc[indices[train_rows:]]\n",
    "\n",
    "# Run the regression and generate predictions for the test set.\n",
    "reg = Ridge(alpha=.1)\n",
    "reg.fit(train, train_upvotes)\n",
    "predictions = reg.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae: 12.2504280089\n",
      "mae: 14.4016602582\n"
     ]
    }
   ],
   "source": [
    "# Use mean absolute error as an error metric.\n",
    "mse_2 = sum(abs(predictions - test_upvotes)) / len(predictions)\n",
    "print(\"mae:\", mse_2)\n",
    "\n",
    "# As a baseline, use the average number of upvotes\n",
    "average_upvotes = sum(test_upvotes)/len(test_upvotes)\n",
    "\n",
    "mse_22 = sum(abs(average_upvotes - test_upvotes)) / len(predictions)\n",
    "print(\"mae:\", mse_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error is about 12.25 upvotes, which means that, on average, the prediction is 12.25 upvotes away from the actual number of upvotes.\n",
    "\n",
    "This method estimates better than using without scikit-learn. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
